<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reward smoothing improves performance of model-based RL on sparse-reward tasks.">
  <meta name="keywords" content="DreamSmooth, MBRL, Dreamer, Reward, Smoothing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DreamSmooth: Improving MBRL via Reward Smoothing</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DreamSmooth</h1>
          <h1 class="title is-4 publication-subtitle">Improving Model-based Reinforcement Learning via Reward Smoothing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/vint-1">Vint Lee</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://youngwoon.github.io/">Youngwoon Lee</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <!-- TODO: update the links -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://vint-1.github.io/dreamsmooth/" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://vint-1.github.io/dreamsmooth/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://vint-1.github.io/dreamsmooth/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.png" class="teaser-image">
      <h2 class="subtitle has-text-centered">
        <span class="technique-name">Dreamsmooth</span> achieves better performance by improving reward prediction on sparse-reward tasks.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: 
              planning actions by generating imaginary trajectories with predicted rewards.
              Despite its success, we found surprisingly that reward prediction is often a bottleneck of MBRL, 
              especially for sparse rewards that are challenging (or even ambiguous) to predict.
            </p>
            <p>
              Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, 
              <span class="technique-name">DreamSmooth</span>, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. 
            </p>
            <p>
              We empirically show that <span class="technique-name">DreamSmooth</span> achieves state-of-the-art performance on long-horizon sparse-reward tasks 
              both in sample efficiency and final performance without losing performance on common benchmarks, 
              such as Deepmind Control Suite and Atari benchmarks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">

    <!-- Method Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <!-- TODO add figures for these -->
        <!-- Current Algorithms. -->
        <h3 class="title is-4">Reward Prediction in Current Algorithms</h3>
        <div class="content has-text-justified">
          <p>
            State-of-the-art MBRL algorithms like DreamerV3 and TD-MPC use reward models to predict the rewards
            that an agent would have obtained for some imagined trajectory. 
            The predicted rewards are then used to derive a policy — by training an actor-critic or planning.
            An accurate reward model is therefore vital to MBRL — 
            reward estimates that are too high will cause an agent to choose actions that perform poorly in reality,
            and estimates that are too low will lead an agent to ignore high rewards.
          </p>
          <p>
            However, we find that in many sparse-reward environments, especially those with
            <b>partial observability</b> or <b>stochastic rewards</b>,
            reward prediction is surprisingly challenging.
            Specifically, the squared-error loss function used in many algorithms require reward models to
            predict sparse rewards at the exact timestep, which is difficult in many environments, even for humans.
          </p>
          <p>
            In such cases, predicting a sparse reward even one timestep too early or too late incurs a large loss,
            more than simply predicting no reward at all timesteps.
          </p>
          <p>
            The model therefore minimizes loss by frequently omitting sparse rewards from its predictions.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            More importantly, we find that the poor reward predictions significantly impact task performance.
            In the earth-moving task, for instance, the goal is to move large rocks from the pile into the dumptruck.
            Large sparse rewards are given for successfully dumping rocks, and small dense rewards are provided 
            for moving rocks towards the dumptruck to accelerate learning.
            Here, the agent behaves as though the large sparse rewards did not exist, successfully picking up and 
            moving the rocks towards the dumptruck, while failing to reliably dump the rocks accurately.
          </p>
        </div>
        <br/>
        <!--/ Current Algorithms. -->

        <!-- Our Solution. -->
        <h3 class="title is-4">Our Solution</h3>
        <div class="content has-text-justified">
          <p>
            We propose <span class="technique-name">DreamSmooth</span>, 
            which performs temporal smoothing of the rewards obtained in each rollout before adding them to the replay buffer. 
            By allowing the reward model to predict rewards that are off from the ground truth by a few timesteps without incurring large losses, 
            our method makes learning easier, especially when rewards are ambiguous or sparse.
          </p>
          <p>
            Our method is extremely simple, requiring <b>only several lines of code changes</b> to existing algorithms,
            while incurring <b>minimal overhead</b>.
          </p>
          <p>
            In this work, we investigate three popular smoothing functions: 
            Gaussian, exponential moving average (EMA), and uniform smoothing.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Our Solution. -->

      </div>
    </div>
    <!--/ Method Overview. -->

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <div class="content has-text-justified">
          <p>
            On a number of sparse-reward tasks, our method outperforms DreamerV3, 
            with task completion rate up to 3x higher in a modified Robodesk environment.
          </p>
        </div>
        <!-- TODO add figures -->

        <!-- Other Algorithms. -->
        <h3 class="title is-4">Reward Smoothing in Other Algorithms</h3>
        <div class="content has-text-justified">
          <p>
            We find that reward smoothing can also improve the performance of TD-MPC, 
            allowing the algorithm to solve the Hand task where it otherwise could not.
            This suggests that <span class="technique-name">DreamSmooth</span> can be useful in a broad range of MBRL algorithms that use a reward model. 
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Other Algorithms. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<!--/ TODO update this -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{vint2023dreamsmooth,
  author    = {Vint Lee and Pieter Abbeel and Youngwoon Lee},
  title     = {DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing},
  journal   = {????},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/vint-1" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
